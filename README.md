# ViT-Notes
This is a repository for recoding the knowledge during Vision Transformer learning.

## Prerequisites


## Update Log
`models/transformer.py` references [[1]](#reference) to reproduce the architecture.

`models/vit_mnist.py` references [[2]](#reference) and [[3]](#reference) to reproduce and train a ViT model on MNIST dataset.<br>
<sub>*Modify the architecture based on [[3]](#reference) to accelerate the efficiency and maintain the accuracy*</sub>


## Reference
[1] **Transformer**: Attention Is All You Need (2017 NeurIPS) [ðŸ“•](https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)

The Annotated Transformer [ðŸ”—](https://nlp.seas.harvard.edu/annotated-transformer/)

[2] **ViT**: An Image is Worth 16Ã—16 Words: Transformers for Image Recognition at Scale (2021 ICLR) [ðŸ“•](https://arxiv.org/pdf/2010.11929/1000)
Illustrated Vision Transformers [ðŸ”—](https://medium.com/analytics-vidhya/illustrated-vision-transformers-165f4d0c3dd1) Vision Transformers from Scratch(Pytorch) [ðŸ”—](https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c)
