# ViT-Notes
This is a repository for recoding the knowledge during Vision Transformer learning.

## Prerequisites


## Update Log
`models/transformer.py` references [[1]](#reference) to reproduce the architecture.

`models/vit_mnist.py` references [[2]](#reference) and [[3]](#reference) to reproduce and train a ViT model on MNIST dataset.<br>
<sub>*Modify the architecture based on [[3]](#reference) to accelerate the efficiency and maintain the accuracy*</sub>


## Reference
### ðŸ“•Paper
[**Transformer**]((https://proceedings.neurips.cc/paper_files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf)): *Attention Is All You Need* (**2017 NeurIPS**) 

[**ViT**](https://arxiv.org/pdf/2010.11929/1000): *An Image is Worth 16Ã—16 Words: Transformers for Image Recognition at Scale* (**2021 ICLR**) 

### ðŸ”—NoteBook
[[1]](https://nlp.seas.harvard.edu/annotated-transformer/) The Annotated Transformer [[2]](https://medium.com/analytics-vidhya/illustrated-vision-transformers-165f4d0c3dd1)  Illustrated Vision Transformers 

[[3]](https://medium.com/@brianpulfer/vision-transformers-from-scratch-pytorch-a-step-by-step-guide-96c3313c2e0c) Vision Transformers from Scratch (Pytorch) 
